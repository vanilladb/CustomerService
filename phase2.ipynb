{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# phase 1 分完重點後，切換到對應的 thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "OpenAI.api_key = os.getenv('OPENAI_API_KEY')\n",
    "print(OpenAI.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "csv_file = 'data\\embeddingsV3_3072.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "data = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatL2(data.shape[1])\n",
    "index.add(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar(input):\n",
    "    response = client.embeddings.create(input=input, model=\"text-embedding-3-large\", dimensions=3072)\n",
    "    question = np.array(response.data[0].embedding, ndmin=2)\n",
    "    k = 3\n",
    "    distances, indices = index.search(question, k)\n",
    "    csv_question = 'data\\manual.csv'\n",
    "    qdf = pd.read_csv(csv_question)\n",
    "    qarray = qdf.to_numpy()\n",
    "    answer = [(qarray[i], float(dist)) for dist, i in zip(distances[0], indices[0])]\n",
    "    history = \"\"\n",
    "    Qcount = 0\n",
    "    for i in range(0,k):\n",
    "        Qcount += 1\n",
    "        history = history +str(i+1)+\". \"+ answer[i][0][0]+\"\\n\"+answer[i][0][1]+\"\\n\"\n",
    "    return Qcount, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_App = \"1.You are InApp chatting bot from Flora customer service. 2.When user ask a question, You must reply based on 3 history questions and answers. 3.If 3 history questions and answers not related to my question, you must reply only a special mark:'*'.\"\n",
    "instruction_App_check = \"1.You are InApp chatting bot from Flora customer service. 2.When user ask a question, You must reply based on 3 history questions and answers. 3.If 3 history questions and answers not related to my question, you must reply only a special mark:'*'. 4.when you encounter question concerning email, you must check email fitting RFC5322 standard or not. if not, ask again\"\n",
    "instruction_Email = \"You are email reply bot from Flora customer service. You must answer based on the history questions and answers from Flora's database. If there is 0 history question and answer, you must reply only a special mark:'*'\"\n",
    "instruction_Comment = \"You are Apple Appstore comment reply bot from Flora customer service. You must answer based on the history questions and answers from Flora's database. If there is 0 history question and answer, you must reply only a special mark:'*'\"\n",
    "\n",
    "instruction = \"\"\n",
    "if channel == 1 :\n",
    "    instruction = instruction_App\n",
    "elif channel == 2 :\n",
    "    instruction = instruction_Email\n",
    "elif channel == 3 :\n",
    "    instruction = instruction_Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name = \"Flora's Customer Service\",\n",
    "    instructions = instruction,\n",
    "    model = \"gpt-3.5-turbo-1106\"\n",
    ")\n",
    "\n",
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar = get_similar(input)\n",
    "prompt = \"Here is \"+str(similar[0])+\" history questions and answers similar to this question according to Flora's database\\n\"+similar[1]+\"Here is my question:\\n\"+input\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id = thread.id,\n",
    "    role = \"user\",\n",
    "    content = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "    thread_id = thread.id,\n",
    "    assistant_id = assistant.id\n",
    ")\n",
    "\n",
    "while run.status == \"queued\" or run.status == \"in_progress\":\n",
    "    run = client.beta.threads.runs.retrieve(\n",
    "        thread_id = thread.id,\n",
    "        run_id = run.id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = client.beta.threads.messages.list(\n",
    "    thread_id = thread.id,\n",
    "    order = 'asc'\n",
    ")\n",
    "\n",
    "for i in messages.data[-10:]:\n",
    "    print(i.content[0].text.value)\n",
    "    print(\"==========\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VScode-env-3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
